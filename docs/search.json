[
  {
    "objectID": "phylogenomics.html#lecture",
    "href": "phylogenomics.html#lecture",
    "title": "14  Introduction to Phylogenomics",
    "section": "14.1 Lecture",
    "text": "14.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "phylogenomics.html#practical",
    "href": "phylogenomics.html#practical",
    "title": "14  Introduction to Phylogenomics",
    "section": "14.2 Practical",
    "text": "14.2 Practical\n\n14.2.1 Preparation\nThe data and conda environment .yaml file for this practical session can be downloaded from here: https://doi.org/10.5281/zenodo.6983184. See instructions on page.\nChange into the session directory\ncd phylogenomics/\nThe data in this folder should contain an alignment (snpAlignment_session5.fasta) and a txt file with the ages of the samples that we are going to be working with in this session (samples.ages.txt)\nLoad the conda environment.\nconda activate phylogenomics\n\n\n14.2.2 Visualize the sequence alignment\nIn this practical session, we will be working with an alignment produced as you learned in the practical Genome mapping.\n\n\n\n\n\n\nWhat is in the data?\n\n\n\n\nthe alignment is a SNP alignment (it contains only the variable genomic positions, not the full genomes)\nit contains 32 Yersinia pestis sequences and 1 Yersinia pseudotuberculosis sequence which can be used as an outgroup\nin this practical, we will investigate the phylogenetic position of four prehistorical Y. pestis strains that we have recently discovered: KZL002, GZL002, CHC004 and VLI092\n\n\n\nWe start by exploring the alignment in MEGA. Open the MEGA desktop application and load the alignment by clicking on File -&gt; Open A File/Session -&gt; Select the snpAligment_session5.fasta.\n\nIt will you ask what you want to do with the alignment. In MEGA you can also produce an alignment, however, since our sequences are already aligned we will press on Analyze.\nThen we will select Nucleotide Sequences since we are working with a DNA alignment. Note that MEGA can also work with Protein Sequences as well as Pairwise Distance Matrix (which we will cover shortly). In the same window, we will change the character for Missing Data to N and click in OK.\n\nA window would open up asking if our alignment contains protein encoding sequences, and we will select No.\n\n\n\n\n\n\nTip\n\n\n\nIf you had protein encoding sequences, you would have selected Yes. This will allow you to treat different positions with different evolutionary modes depending on their codon position. One can do this to take in account that the third codon position can change to different nucleotides without resulting in a different amino acid, while position one and two of the codon are more restricted.\n\n\nTo explore the alignment, you will then click on the box with TA\n\nYou will see an alignment containing sequences from the bacterial pathogen Yersinia pestis. Within the alignment, we have four sequences of interest (KZL002, GZL002, CHC004 and VLI092) that date between 5000-2000 years Before Present (BP), and we want to know how they relate to the rest of the Yersinia pestis genomes in the alignment.\n\nQuestions:\nHow many sequences are we analysing?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe are analysing 33 sequences.\n\n\n\nWhat are the Ns in the sequences?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey represent positions where we have missing data. We told MEGA to encode missing positions as N\n\n\n\nWhat do you think the dots represent?\n\n\n\n\n\n\nTip\n\n\n\nThe first line is a consensus sequence: it indicates the nucleotide supported by the majority of the sequences in the alignment (90% of the sequences should agree, otherwise an N is displayed)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey represent positions that are same as the consensus\n\n\n\nOnce you know this, can you already tell by looking at the alignment which sequence is the most singular (scroll down)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe can easily see that the last sequence in the alignment (Y. pseudotuberculosis) contains more disagreements to the consensus. This is normal since this is the only genome not belonging to the Y. pestis species: we will use it as an outgroup\n\n\n\n\n\n14.2.3 Distance-based phylogeny: Neighbour Joining\nThe Neighbour Joining (NJ) method is an agglomerative algorithm which can be used to derive a phylogenetic tree from a pairwise distance matrix. In essence, this method will be grouping taxa that have the shortest distance together first, and will be doing this iteratively until all the taxa/sequences included in your alignment have been placed in a tree.\nHere are the details of the calculations for a small NJ tree example with 6 taxa:\n\nLuckily, you won’t have to do this by hand since MEGA allows you to build a NJ tree. For that go back to MEGA and click on the Phylogeny symbol and then select Construct Neighbour Joining Tree. In the window that would pop up, you will then chance the Model/Method to p-distance. Then press OK and a window with the calculated phylogenetic tree will pop up.\n\n\n\n\n\n\np-distances?\n\n\n\nA NJ tree can be built from any type of distances. This includes:\n\np-distances (also called raw distances): these are simply the proportion of differences between sequences\ncorrected distances: these are based on an underlying substitution model (JC69, K80, GTR…) and account for multiple mutations at the same sites (which would result in only one visible difference)\np-distances and corrected distances should be similar when the number of mutations is low compared to the genome length\n\n\n\n\nSince the tree is not easily visualised in MEGA, we will export it in newick format (an “standarised” format to write a tree in a computer-readable form) and explore our tree in FigTree. This tool has a better interface for visually manipulating trees and allows us to interact with the phylogenetic tree.\n\nTo do that you will click on File, then Export current tree (Newick) and click on Branch Lengths to include those in the newick annotation. When you press OK, a new window with the tree in newick format will pop up and you will then press File -&gt; Save and saved it as NJ_tree.nwk (If you are doing this for your own project, please give your files informative names).\nAs said above, we will explore own NJ tree in FigTree. Open the software and then open the NJ tree by clicking on File -&gt; Open and selecting the file with the NJ tree NJ_tree.nwk\n\nNote that even though a root is displayed by default in FigTree, NJ trees are actually unrooted. We know that Yersinia pseudotuberculosis (labelled here as Y. pseudotuberculosis) is an outgroup to Yersinia pestis. You can reroot the tree by selecting Y.pseudotuberculosis and pressing Reroot.\n\nNow we have a rooted tree.\nQuestions:\nHow much time did the NJ-tree calculation take?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~1 second\n\n\n\nHow many leaves/tips has our tree?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n33, i.e. the number of sequences in our SNP alignment.\n\n\n\nWhere are our taxa of interest? (KZL002, GZL002, CHC004 and VLI092)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThey all fall ancestral to the rest of Yersinia pestis in this tree.\n\n\n\nDo they form a monophyletic group (a clade)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes, they form a monophyletic group. We can also say that this group of prehistoric strains form their own lineage.\n\n\n\n\n\n14.2.4 Character-based phylogenetic methods: maximum parsimony and probabilistic approaches\nCharacter-based methods are not based on pairwise distances but rather model the complete evolution of each character (e.g. DNA nucleotides at each position) along the phylogenetic tree.\nOne of these methods is maximum parsimony and it consists in choosing the tree that underlies an evolutionary history with the least number of character changes.\n\n\nOther types of character-based methods which are more commonly used today are probabilistic methods. In general, these are statistical techniques that are based on probabilistic models under which the data that we observe is generated following a probability distribution depending on a set of parameters which we want to estimate. The probability of the data given the model parameters is called the likelihood.\n\nQuestion:\nIn a phylogenetic probabilistic model, what are the data and what are the parameters?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn a phylogenetic probabilistic model, the data is the sequence alignment and the parameters, are:\n\nthe parameters of the chosen substitution model (substitution rates and base frequencies)\nthe phylogenetic tree\n\n\n\n\n\n\n14.2.4.1 Maximum likelihood estimation and bootstrapping\nOne way we can make inferences from a probabilistic model is by finding the combination of parameters which maximises the likelihood. These parameter values are called maximum likelihood (ML) estimates. We are usually not able to compute the likelihood value for all possible combinations of parameters and have to rely on heuristic algorithms to find the maximum likelihood estimates.\n\nThe Maximum likelihood estimates are point estimates, i.e. single parameter values (for example, a tree), which does not allow to measure uncertainty. A classic method to measure the uncertainty in ML trees is bootstrapping, which consists in repeatedly “disturbing” the alignment by masking sites randomly and estimating a tree from each of these bootstrap alignments.\n\nFor each clade in the ML tree, a bootstrap support value is computed which corresponds to the proportion of bootstrap trees containing the clade. This gives an indication of how robustly the clade is supported by the data (i.e. whether it holds even after disturbing the dataset). Bootstrapping can be used to measure the topology uncertainty of trees estimated with any inference method.\n\n\n\n\n\n\nNote\n\n\n\nBootstrapping can be used to measure incertainty with any type of inference method, including distance methods\n\n\nLet’s make our own ML tree!\nHere is a command to estimate an ML phylogenetic tree together with bootstraps using RAxML (you may find the list of parameters in the RAxML manual):\nraxmlHPC-PTHREADS -m GTRGAMMA -T 3 -f a -x 12345 -p 12345 -N autoMRE -s snpAlignment_session5.fasta -n full_dataset.tre\nHere is the meaning of the chosen parameters:\n\nOnce the analysis has been completed, you can open the tree using Figtree (RAxML_bipartitions.full_dataset.tre file, change “label” to “bootstrap support” at the prompt).\n\nThe tree estimated using this model is a substitution tree (branch lengths represent genetic distances in subst./site). As for the NJ tree,it is not oriented in time: this is an unrooted tree (displayed with a random root in Figtree). You can reroot the tree in Figtree using Y. pseudotuberculosis as an outgroup, as previously.\nQuestions:\nCan you confirm the position of our genomes of interest (KZL002, GZL002, CHC004 and VLI092)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes. Just as in the NJ tree, they form a clade which is basal to the rest of the Y. pestis diversity.\n\n\n\nIs that placement well-supported? (look at the bootstrap support value: click on the “Node Labels” box and open the drop-down menu, change “Node ages” to “bootstrap support”)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe placement is strongly supported as indicated by a bootstrap support of 100% for this clade (it is not very easy to see, you probably need to zoom in a bit)\n\n\n\n\nYou can notice that the phylogeny is difficult to visualize due to the long branch leading to Y. pseudotuberculosis. Having a very distant outgroup can also have deleterious effects on the estimated phylogeny (due to the so-called “long branch attraction” effect). We can construct a new phylogeny after removing the outgroup:\n\ngo back to the alignment in mega, unclick Y.pseudotuberculosis, and export in fasta format (“Data” -&gt; “Export Data” -&gt; change “Format” to “Fasta” and click “Ok”; you can save it as: “snpAlignment_without_Ypseudo.fas”)\nrun raxml on this new alignment (change input to “snpAlignment_without_Ypseudo.fas” and output prefix to “without_Ypseudo” in the commandline)\nopen the bipartition… file in figtree and reroot the tree based on the knowledge we have gained previously: place the root on the branch leading to the prehistoric Y. pestis strains (KZL002, GZL002, CHC004 and VLI092).\n\n\nLastly, we will export the rooted tree from figtree: File -&gt; Export trees -&gt; select the “save as currently displayed” box and save as “ML_tree_rooted.tre”\n\n\n\n14.2.4.2 Estimating a time-tree using Bayesian phylogenetics (BEAST2)\nNow, we will try to use reconstruct a phylogeny in which the branch lengths do not represent a number of mutations but instead represent the time of evolution. To do so, we will use the ages of ancient genomes (C14 dates) to calibrate the tree in time. This assumes a molecular clock hypothesis in which substitutions occur at a rate that is relatively constant in time so that the time of evolution can be estimated based on the number of substitutions.\n\n\n\n\n\n\nNote\n\n\n\nA great advantage of ancient pathogen genomes is that they provide key calibration points to estimate molecular clocks and dated phylogenies. This is much more difficult to do with modern data alone.\n\n\nWe will estimate a time tree from our alignment using Bayesian inference as implemented in the BEAST2 software. Bayesian inference is based on a probability distribution that is different from the likelihood: the posterior probability. The posterior probability is the probability of the parameters given the data. It is easier to interpret than the likelihood because it directly contains all the information about the parameters: point estimates such as the median or the mean can be directly estimated from it, but also percentile intervals which can be used to measure uncertainty.\n\nThe Bayes theorem tells us that is proportional to the product of the likelihood and the “prior” probability of the data:\n\nTherefore, for Bayesian inference, we need to complement our probabilistic model with prior distributions for all the parameters. Because we want to estimate a time tree, we also add another parameter: the molecular clock (average substitution rate in time units).\n\nTo characterize the full posterior distribution of each parameter, we would need in theory to compute the posterior probability for each possible combination of parameters. This is impossible, and we will instead use an algorithm called Markov chain Monte Carlo (MCMC) to approximate the posterior distribution. The MCMC is an algorithm which iteratively samples values of the parameters from the posterior distribution. Therefore, if the MCMC has run long enough, the (marginal) posterior distribution of the parameters can be approximated by a histogram of the sampled values.\n\n\n14.2.4.2.1 Set up a BEAST2 analysis\n\n\n\n\n\n\nTip\n\n\n\nThe “taming the beast” website has great tutorials to learn setting a BEAST2 analysis. In particular, the “Introduction to BEAST2”, “Prior selection” and “Time-stamped data” are good starts.\n\n\nThe different components of the BEAST2 analysis can be set up in the program BEAUti:\n\nOpen BEAUTi and set up an analysis as followed:\n\nload the alignment without Y. pseudotuberculosis in the “Partitions” tab (“File” -&gt; “Import alignment”; select “nucleotide”)\n\n\n\nset the sampling dates in the “Tip dates” tab:\n\nselect “Use tip dates”\nclick on “Auto-configure” -&gt; “read from file” and select the sample_dates.txt file\nchange “Since some time in the past” to “Before present”\n\n\n\n\nselect the substitution model in the “Site model” tab:\n\nchose a GTR model\nuse 4 Gamma categories for the Gamma site model: this is to account for variations of the substitution rate accross sites (site=nucleotide position)\n\n\n\n\nchoose the molecular clock model in the “Clock model” tab:\n\nuse a relaxed clock lognormal model (this is to allow for some variation of the clock rate accross branches)\nchange the initial value of the clock rate to 10-4 substitution/site/year (10-4 can be written 1E-4)\n\n\n\n\nchoose the prior distribution of parameters in the “Priors” tab:\n\nuse a Bayesian Skyline Coalescent tree prior\nchange the mean clock prior to a uniform distribution between 1E-6 and 1E-3 subst/site/year\nleave everything else to default\n\n\n\n\nset up the MCMC in the “MCMC” tab:\n\nuse a chain length of 300M\nsample the monodimensional parameters and trees every 10,000 iterations (“tracelog/treelog” -&gt; “log every”)\n\n\n\n\nsave the analysis setup as an xml file: “File” -&gt; “Save as”; you can name the file “beast_analysis_Y_pestis.xml”\n\nNow that the analysis is setup, we can run it using BEAST:\nbeast beast_analysis_Y_pestis.xml\nOnce the analysis is running two files should have been created and are continuously updated: - the “snpAlignment_without_Ypseudo.log” file which contains the values sampled by the MCMC for various monodimensional parameters such as the clock rate, as well as other values that are a logged along the MCMC such as the posterior probability and the likelihood. - the “snpAlignment_without_Ypseudo.trees” file which contains the MCMC trees sampled by the MCMC\nWhile the analysis is running, you can start reading the next section\n\n\n14.2.4.2.2 Assessing BEAST2 results\n\n\n\n\n\n\nReminder\n\n\n\nWe are using an MCMC algorithm to sample the posterior distribution of parameters. If the MCMC has run long enough, we can use the sampled parameters to approximate the posterior distribution itself. Therefore, we have to check first that the MCMC chain has run long enough.\n\n\nWe can assess the MCMC sampling using the program Tracer. Tracer can read BEAST log files an generate statistics and plots for each of the sampled parameters. Most importantly, Tracer provides:\n\ntrace plots: show the sampled parameter values along the MCMC run. Trace plots are a very useful MCMC diagnostic tool.\n\n\nThe first thing that one needs to assess is wether the MCMC has passed the so called “burn-in” phase. The MCMC starts with a random set of parameters and will take some time to reach a zone of high posterior probability density. The parameter values that are sampled during this initial phase are usually considered as noise and discarded (by default, tracer discards the first 10% of samples). The burn-in phase can be visualize on trace plots as an initial phase during which the posterior probability of sampled parameters is constantly increasing before reaching a plateau:\n\nOnce the burn-in phase is passed, one can look at the trace plots to assess if the parameters are sampled correctly and long enough. In most cases, when this is the case, the trace should be quite dense and oscillating around a central value (nice trace plots should look like “hairy caterpillars”). In the figure below, the trace on the left doesn’t look good, the one on the right does:\n\n– ESS values: tracer also calculates effective sample sizes (ESS) for each of the sampled parameters. ESSs are estimates of the number of sampled parameter values after correcting for auto-correlation along the MCMC. As a rule of thumb, one usually considers that an MCMC as run long enough if all parameter’s ESS are &gt; 200. Note that if the trace looks like a hairy caterpillar, the corresponding ESS value should be high.\n\n– Parameter estimates: Tracer also provides statistics and plots to explore the posterior distribution of the parameters. These should be considered only if the trace plot and ESS values look fine. In the “Estimates” tab, after selecting the chosen parameter in the left panel, the upper-right panel shows point estimates (mean, median) and measures of uncertainty (95% HPD interval), and the bottom-right panel shows a histogram of the sampled value:\n\nLet’s now load the “snpAlignment_without_Ypseudo.log” file into trace (you can slide the file into the top-left panel). Note that one can load a BEAST2 log file into tracer even if the analysis is still running. This allows to assess if the MCMC is running correctly or has run long enough before it’s completed.\nQuestion:\nHas the MCMC run long enough?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou have probably let you analysis run for 10-20 mins before looking at the log file, and this is definitely not sufficient: the burnin phase has recently been passed, the trace plots do not look very dense and ESS values are low. It would probably take a few hours for the analysis to complete. Luckily we have run the analysis in advance and saved the log files for you “beast_analysis_Y_pestis_complete.log” and “beast_analysis_Y_pestis_complete.trees”\n\n\n\nYou can now load the “beast_analysis_Y_pestis_complete.log” file into Tracer.\nQuestions:\nHas the MCMC run long enough?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! The trace plots look good and all ESSs are &gt; 200\n\n\n\nWhat is your mean estimate of the clock rate (ucld mean)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\n\n\n14.2.4.2.3 MCC tree\nSince we are working in a Bayesian framework, we do not obtain a single phylogenetic tree as with Maximum likelihood, but a large set of trees which should be representative of the posterior distribution. In contrast with monodimensional parameters, a tree distribution cannot be easily summarized with mean or median estimates. Instead, we need to use specific tree-summarizing techniques. One of the most popular is the maximum clade credibility (MCC) tree, which works as follow:\n\n\nFor any node in any of the sampled trees, compute a posterior support: the proportion of trees in the sample which contain the node\n\n\nSelect the MCC tree: this is the tree which the product of node posterior supports is the highest\n\n\nCalculate node/branch statistics on the MCC tree: typically, the mean/median estimates and HPD interval of node ages are calculated from the full tree sample and annotated on the MCC tree\n\n\n\nLet’s generate an MCC tree from our tree sample. We can do this using the TreeAnnotator software, which has both a commnadline and graphical interface. Let’s use the commandline here and run the following (using a burn-in of 10%):\n treeannotator -burnin 10 beast_analysis_Y_pestis_complete.trees beast_analysis_Y_pestis_complete.MCC.tre\nOnce this is completed, we can open the MCC tree with figtree. Let’s then add a few elements to the plot:\n\n\nTick the “Scale Axis” box, unfold the corresponding menu, and select “Reverse axis” (now the timescale is in years BP)\n\n\nTick the “Node Labels” box, unfold the corresponding menu, and select “Display: posterior”. The posterior support of each node is now displayed. Note that the support value is a proportion (1=100%)\n\n\nTick the “Node Bars” box, unfold the corresponding menu, and select “Display: height_95%_HPD”. The 95% HPD intervals of node ages are now displayed.\n\n\n\nQuestions:\nIs the root of the tree consistent with what we found previously?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes! The root is placed between our prehistoric strains and the rest of Y. pestis strains. Note that this time we didn’t have to use an outgroup because we estimated a time-tree: the root is identified as the oldest node in the tree.\n\n\n\nWhat is your estimate for the age of the most recent common ancestor of all Y. pestis strains?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n~5500 years BP (HPD 95%: ~7500-4500 years BP)\n\n\n\n\n\n\n14.2.4.3 Bonus: Temporal signal testing\nIt is a good practice to assess if the genetic sequences that we analyse do indeed behave like molecular clocks before trying to estimate a time tree. A classic test to do this is called root-to-tip regression, which consists in verifying that the oldest a sequence is, the closer it should be to the root because there was less time for mutations to accumulate before this sequence was sampled. The correlation between sample age and distance to the root (root-to-tip regression) can be assessed using a rooted substitution tree and the program tempest:\n\nopen tempest and load the re-rooted ML tree that we produced previously\nclick on “import dates” in the “sample dates” tab, select the sample_age.txt file, and then change to “dates specified as years before the present”\nlook at the root-to-tip regression: is there a positive correlation?"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#lecture",
    "href": "ancient-metagenomic-pipelines.html#lecture",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.1 Lecture",
    "text": "16.1 Lecture\n\n\nPDF version of these slides can be downloaded from here."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#introduction",
    "href": "ancient-metagenomic-pipelines.html#introduction",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.2 Introduction",
    "text": "16.2 Introduction\nA pipeline is a series of linked computational steps, where the output of one process becomes the input of the next. Pipelines are critical for managing the huge quantities of data that are now being generated regularly as part of ancient DNA analyses. Today we will discuss one option for managing computational analyses of ancient next-generation sequencing datasets, nf-core/eager. Keep in mind that other tools, like the Paleomix pipeline, can also be used for similar applications."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "href": "ancient-metagenomic-pipelines.html#what-is-nf-coreeager",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.3 What is nf-core/eager?",
    "text": "16.3 What is nf-core/eager?\nnf-core/eager is a computational pipeline specifically designed for preprocessing and analysis of ancient DNA data. It is a reimplementation of the previously published EAGER (Efficient Ancient Genome Reconstruction) pipeline (Peltzer et al. 2016) using Nextflow. The nf-core/eager pipeline was designed with the following aims in mind:\n\nPortability- In order for our analyses to be reproducible, others should be able to easily implement our computational pipelines. nf-core/eager is highly portable, providing easy access to pipeline tools and facilitating use across multiple platforms. nf-core eager utilizes Docker, Conda, and Singularity for containerization, enabling distrubition of the pipeline in a self-contained bundle containing all the code, packages, and libraries needed to run it.\nReproducibility- nf-core/eager uses custom configuration profiles to specify both HPC-level parameters and analyses-specific options. These profiles can be shared alongside your publication, making it easier for others to reproduce your methodology!\nNew Tools- Finally, nf-core/eager includes additional, novel methods and tools for analysis of ancient DNA data that were not included in previous versions. This is especially good news for folks interested in microbial sciences, who can take advantage of new analytical pathways for metagenomic analysis and pathogen screening."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#steps-in-the-pipeline",
    "href": "ancient-metagenomic-pipelines.html#steps-in-the-pipeline",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.4 Steps in the pipeline",
    "text": "16.4 Steps in the pipeline\n\nA detailed description of steps in the pipeline is available as part of nf-core/eager’s extensive documentation. For more information, check out the usage documentation here.\nBriefly, nf-core/eager takes standard input file types that are shared across the genomics field, including raw fastq files, aligned reads in bam format, and a reference fasta. nf-core/eager can perform preprocessing of this raw data, including adapter clipping, read merging, and quality control of adapter-trimmed data. Note that input files can be specified using wildcards OR a standardized tsv format file; the latter facilitates streamlined integration of multpile data types within a single EAGER run! More on this later.\nnf-core/eager facilitates mapping using a variety of field-standard alignment tools with configurable parameters. An exciting new addition in nf-core/eager also enables analysis of off-target host DNA for all of you metagenomics folks out there. Be sure to check out the functionality available for metagenomic profiling (blue route in the ‘tube map’ above).\nnf-core/eager incorporates field-standard quality control tools designed for use with ancient DNA so that you can easily evaluate the success of your experiments. Multiple genotyping approaches and additional analyses are available depending on your input datatype, organism, and research questions. Importantly, all of these processes generate data that we need to compile and analyze in a coherent way. nf-core eager uses MultiQC. to create an integrated html report that summarizes the output/results from each of the pipeline steps. Stay tuned for the practical portion of the walkthrough!"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#how-to-build-an-nf-coreeager-command-a-practical-introduction",
    "href": "ancient-metagenomic-pipelines.html#how-to-build-an-nf-coreeager-command-a-practical-introduction",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.5 How to build an nf-core/eager command: A practical introduction",
    "text": "16.5 How to build an nf-core/eager command: A practical introduction\nFor the practical portion of the walkthrough, we will utilize sequencing data from four aDNA libraries, which you should have already downloaded from NCBI. If not, please see the Preparation section above.\nThese four libraries come from from two ancient individuals, GLZ002 and KZL002. GLZ002 comes from the Neolithic Siberian site of Glazkovskoe predmestie adn was radiocarbon dated to 3081-2913 calBCE. KZL002 is an Iron Age individual from Kazakhstan, radiocarbon dated to 2736-2457 calBCE. Both individuals were infected with the so-called ‘Stone Age Plague’ of Yersinia pestis, and libraries from these individuals were processed using hybridization capture to increase the number of Y. pestis sequences available for analysis.\nOur aims in the following tutorial are to:\n\nPreprocess the fastq files by trimming adapters and merging paired-end reads\nAlign reads to the Y. pestis reference and compute the endogenous DNA percentage\nFilter the aligned reads to remove host DNA\nRemove duplicate reads for accurate coverage estimation and genotyping\nMerge data by sample and perform genotyping on the combined dataset\nReview quality control data to evaluate the success of the previous steps\n\nLet’s get started!\nFirst, activate the conda environment that we downloaded during setup:\nconda activate git-eager\nNext, download the latest version of the nf-core/eager repo (or check for updates if you have a previously-installed version):\nnextflow pull nf-core/eager\nFinally, we will build our eager command:\nnextflow run nf-core/eager \\ #Tells nextflow to execute the EAGER pipeline\n-r 2.4.5 -ds1l \\ #Specifies which pipeline and Nextflow versions to run for reproducibility\n-profile conda  \\ #Profiles configure your analysis for specific computing environments/analyses\n--fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna \\ #Specify reference in fasta format\n--input ancientMetagenomeDir_eager_input.tsv \\ #Specify input in tsv format or wildcards\n--run_bam_filtering --bam_unmapped_type fastq \\ #Filter unmapped reads and save in fastq format\n--run_genotyping --genotyping_tool ug --gatk_ug_out_mode EMIT_ALL_SITES \\ #Run genotyping with the GATK UnifiedGenotyper\n--run_bcftools_stats #Generate variant calling statistics\nFor full parameter documentation, click here.\nAnd now we wait…"
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#top-tips-for-nf-coreeager-success",
    "href": "ancient-metagenomic-pipelines.html#top-tips-for-nf-coreeager-success",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.6 Top Tips for nf-core/eager success",
    "text": "16.6 Top Tips for nf-core/eager success\n\nScreen sessions\n\nDepending on your input data, infrastructre, and analyses, running nf-core/eager can take hours or even days. To avoid crashed due to loss of power or network connectivity, try running nf-core/eager in a screen or tmux session:\nscreen -R eager\n\nMultiple ways to supply input data\n\nIn this tutorial, a tsv file to specify our input data files and formats. This is a powerful approach that allows nf-core eager to intelligently apply analyses to certain files only (e.g. merging for paired-end but not single-end libraries). Check out the contents of our tsv input file using the following command:\ncat ancientMetagenomeDir_eager_input.tsv\nInputs can also be specified using wildcards, which can be useful for fast analyses with simple input data types (e.g. same sequencing configuration, file location, etc.).\nnextflow run nf-core/eager -r 2.4.5 -ds1l -profile conda --fasta ../reference/GCF_001293415.1_ASM129341v1_genomic.fna\n--input \"data/*fastq.gz\" &lt;...&gt;\nSee the online nf-core/eager documentation for more details.\n\nGet your MultiQC report via email\n\nIf you have GNU mail or sendmail set up on your system, you can add the following flag to send the MultiQC html to your email upon run completion:\n--email \"your_address@something.com\"\n\nCheck out the EAGER GUI\n\nFor folks who might be less comfortable with the command line, check out the nf-core/eager GUI! The GUI also provides a full list of options with short explanations for those interested in learning more about what the pipeline can do.\n\nWhen something fails, all is not lost!\n\nWhen individual jobs fail, nf-coreager will try to automatically resubmit that job with increased memory and CPUs (up to two times per job). When the whole pipeline crashes, you can save time and computational resources by resubmitting with the -resume flag. nf-core/eager will retrieve cached results from previous steps as long as the input is the same.\n\nMonitor your pipeline in real time with the Nextflow Tower\n\nRegular users may be interested in checking out the Nextflow Tower, a tool for monitoring the progress of Nextflow pipelines in real time. Check here for more information."
  },
  {
    "objectID": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "href": "ancient-metagenomic-pipelines.html#questions-to-think-about",
    "title": "16  Ancient Metagenomic Pipelines",
    "section": "16.7 Questions to think about",
    "text": "16.7 Questions to think about\n\nWhy is it important to use a pipeline for genomic analysis of ancient data?\nHow can the design of the nf-core/eager pipeline help researchers comply with the FAIR princples for management of scientific data?\nWhat metrics do you use to evaluate the success/failure of ancient DNA sequencing experiments? How can these measures be evaluated when using nf-core/eager for data preprocessing and analysis?"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "resources.html#introduction-to-ngs-sequencing",
    "href": "resources.html#introduction-to-ngs-sequencing",
    "title": "17  Resources",
    "section": "17.1 Introduction to NGS Sequencing",
    "text": "17.1 Introduction to NGS Sequencing\n\nhttps://www.youtube.com/watch?v=fCd6B5HRaZ8\nhttps://emea.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dijk, Erwin L van, Hélène Auger, Yan Jaszczyszyn, and Claude Thermes.\n2014. “Ten Years of Next-Generation Sequencing Technology.”\nTrends in Genetics 30 (9): 418–26. https://doi.org/10.1016/j.tig.2014.07.001.\n\n\nKircher, Martin, Susanna Sawyer, and Matthias Meyer. 2012. “Double\nIndexing Overcomes Inaccuracies in Multiplex Sequencing on the Illumina\nPlatform.” Nucleic Acids Research 40 (1): e3. https://doi.org/10.1093/nar/gkr771.\n\n\nMa, Xiaotu, Ying Shao, Liqing Tian, Diane A Flasch, Heather L Mulder,\nMichael N Edmonson, Yu Liu, et al. 2019. “Analysis of Error\nProfiles in Deep Next-Generation Sequencing Data.” Genome\nBiology 20 (1): 50. https://doi.org/10.1186/s13059-019-1659-6.\n\n\nMeyer, Matthias, and Martin Kircher. 2010. “Illumina Sequencing\nLibrary Preparation for Highly Multiplexed Target Capture and\nSequencing.” Cold Spring Harbor Protocols 2010 (6):\ndb.prot5448. https://doi.org/10.1101/pdb.prot5448.\n\n\nSchuster, Stephan C. 2008. “Next-Generation Sequencing Transforms\nToday’s Biology.” Nature Methods 5 (1): 16–18. https://doi.org/10.1038/nmeth1156.\n\n\nShendure, Jay, and Hanlee Ji. 2008. “Next-Generation\nDNA Sequencing.” Nature Biotechnology 26\n(10): 1135–45. https://doi.org/10.1038/nbt1486.\n\n\nSinha, Rahul, Geoff Stanley, Gunsagar Singh Gulati, Camille Ezran, Kyle\nJoseph Travaglini, Eric Wei, Charles Kwok Fai Chan, et al. 2017.\n“Index Switching Causes ‘Spreading-of-Signal’ Among\nMultiplexed Samples in Illumina HiSeq 4000 DNA\nSequencing.” bioRxiv. https://doi.org/10.1101/125724.\n\n\nSlatko, Barton E, Andrew F Gardner, and Frederick M Ausubel. 2018.\n“Overview of Next-Generation Sequencing Technologies.”\nCurrent Protocols in Molecular Biology / Edited by Frederick M.\nAusubel ... [Et Al.] 122 (1): e59. https://doi.org/10.1002/cpmb.59.\n\n\nValk, Tom van der, Francesco Vezzi, Mattias Ormestad, Love Dalén, and\nKaterina Guschanski. 2019. “Index Hopping on the Illumina\nHiseqX Platform and Its Consequences for Ancient\nDNA Studies.” Molecular Ecology Resources,\nMarch. https://doi.org/10.1111/1755-0998.13009."
  },
  {
    "objectID": "tools.html#introduction-to-r-and-the-tidyverse",
    "href": "tools.html#introduction-to-r-and-the-tidyverse",
    "title": "18  Tools",
    "section": "18.1 Introduction to R and the Tidyverse",
    "text": "18.1 Introduction to R and the Tidyverse\n\nr\nr studio (desktop)\ntidyverse"
  },
  {
    "objectID": "tools.html#introduction-to-python-and-pandas",
    "href": "tools.html#introduction-to-python-and-pandas",
    "title": "18  Tools",
    "section": "18.2 Introduction to Python and Pandas",
    "text": "18.2 Introduction to Python and Pandas\n\npython\njupyter"
  },
  {
    "objectID": "tools.html#introduction-to-github",
    "href": "tools.html#introduction-to-github",
    "title": "18  Tools",
    "section": "18.3 Introduction to Git(Hub)",
    "text": "18.3 Introduction to Git(Hub)\n\ngit (normally installed by default on all UNIX based operating systems e.g. Linux, OSX)"
  },
  {
    "objectID": "tools.html#functional-profiling",
    "href": "tools.html#functional-profiling",
    "title": "18  Tools",
    "section": "18.4 Functional Profiling",
    "text": "18.4 Functional Profiling\n\nr\nr studio (desktop)\ntidyverse\nhumann3"
  },
  {
    "objectID": "tools.html#de-novo-assembly",
    "href": "tools.html#de-novo-assembly",
    "title": "18  Tools",
    "section": "18.5 De novo assembly",
    "text": "18.5 De novo assembly\n\nfastp\nmegahit\nbowtie2\nsamtools\nbioawk\ndiamond\nmetabat2\nmaxbin2\nconcoct\nmetawrap\ncheckm-genome\ngunc\npydamage\nprokka"
  },
  {
    "objectID": "tools.html#genome-mapping",
    "href": "tools.html#genome-mapping",
    "title": "18  Tools",
    "section": "18.6 Genome Mapping",
    "text": "18.6 Genome Mapping\n\nbwa\nigv\ngatk"
  },
  {
    "objectID": "tools.html#phylogenomics",
    "href": "tools.html#phylogenomics",
    "title": "18  Tools",
    "section": "18.7 Phylogenomics",
    "text": "18.7 Phylogenomics\n\nbeast2_\ntracer\ntempest\nmega"
  },
  {
    "objectID": "tools.html#ancient-metagenomic-pipelines",
    "href": "tools.html#ancient-metagenomic-pipelines",
    "title": "18  Tools",
    "section": "18.8 Ancient Metagenomic Pipelines",
    "text": "18.8 Ancient Metagenomic Pipelines\n\nnextflow\nnf-core tools\nnf-core/eager"
  }
]