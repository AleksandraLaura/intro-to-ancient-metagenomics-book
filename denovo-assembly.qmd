---
title: Introduction to _de novo_ Genome Assembly
author: Alexander HÃ¼bner
---

```{r echo = F, message = F}
library(tidyverse)
library(pander)
```


::: {.callout-tip}
For this chapter's exercises, if not already performed, you will need to create the [conda environment](before-you-start.qmd#creating-a-conda-environment) from the `yml` file in the following [link](https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/denovo-assembly.yml) (right click and save as to download), and once created, activate the environment with:

```bash
conda activate denovo-assembly
```
:::

### Lecture

Lecture slides and video from the [2022 edition of the summer school](https://www.spaam-community.org/wss-summer-school/#/2022/README).

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQiyXe-6yn1hfaoibwI8DBYHubQxEd_WEvXte6JqyyjnuY8DXfP64360tRvhmRxOPAoM4Gt6hqKG5ts/embed?start=false&loop=true&delayms=10000" frameborder="0" width="100%" height="400px" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

PDF version of these slides can be downloaded from [here](https://github.com/SPAAM-community/wss-summer-school/raw/main/docs/assets/slides/2022/4c-intro-to-denovoassembly/SPAAM%20Summer%20School%202022%20-%204C%20-%20Genome%20Assembly.pdf).

<iframe width="100%" height="400px" src="https://www.youtube.com/embed/1-o4449Hu4o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Introduction

First of all, what is a **metagenomic** sample? Metagenomic sample is a sample that consists of DNA from more than one source. The number and the type of sources might vary widely between different samples. Typical sources for ancient remains are e.g. the host organism and the microbial species. The important part is that we generally do not know the origin of a DNA molecule prior to analysing the sequencing data generated from the DNA library of this sample. In the example presented in **Figure 1**, our metagenomic sample has DNA from three different sources, here coloured in blue, red, and yellow.


```{r out.width = "80%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/metagenome_assembly_scheme.png")
```

**Figure 1**: **Overview of the ways how to analyse metagenomic sequencing data.**

How can we determine the sources of the DNA that we have in our metagenomic sample? There are three main options whose _pros_ and _cons_ are summarised in **Table 1**.

**Table 1**: **Pros and cons of the major methods for determining the sources of the DNA.**

```{r results = "asis", echo = F}
tibble(method = c("reference-based alignment", "single-genome assembly", "metagenome assembly"),
       pros = c("highly sensitive, applicable to aDNA samples",
                "high-quality genomes from cultivated bacteria",
                "able to recover unknown diversity present in sample"),
       cons = c("requires all sources to be represented in database",
                "not available for ancient DNA samples",
                "highly dependent on preservation of ancient DNA")) %>%
pandoc.table(., split.tables = Inf)
```

Unitl recently, the only option for ancient DNA samples was to take the short-read sequencing data and align them against some known reference genomes. However, this approach is heavily relying on the whether all sources of our samples are represented in the available reference genomes. If a source is missing in the reference database - in our toy example, this is the case for the yellow source (**Figure 1**) -, then we won't be able to detect it using this reference database.

While there is a potential workaround for modern metagenomic samples, _single-genome assembly_ it relies on being able to cultivate a microbial species to obtain an isolate. This is unfeasible for ancient metagenomic samples because there are no more viable microbial cells available that could be cultivated.

Around 2015, a technical revolution started when the first programs, such as MEGAHIT [@LiMegahit2015] and metaSPAdes [@Nurk2017], were published that could successfully perform *de novo* assembly from metagenomic data. Since then, tens of thousands metagenomic samples have been assembled and it was revealed that even well studied environments, such as the human gut microbiome, have a lot of additional microbial diversity that could not be discovered via culturing and isolation [@Almeida2021]. 

The technical advancement of now being able to perform *de novo* assembly on metagenomic samples led to an explosion of studies that analysed samples that were considered almost impossible to study beforehand. For researchers that are exposed to ancient DNA, the imminent question arises: can we apply the same methods to ancient DNA data? In this practical, we will walk through all required steps that are necessary to successfully perform *de novo* assembly from ancient DNA metagenomic sequencing data and show you what you can do once you have obtained the data.

## Practical

### Sample overview

For this practical, I selected a palaeofaeces sample from the study by @Maixner2021, who generated deep metagenomic sequencing data for four palaeofaeces samples that were excavated from an Austrian salt mine in Hallstatt and were associated with the Celtic Iron Age. We will focus on the youngest sample, **2612**, which was dated to be just a few hundred years old (**Figure 2**).

```{r out.width = "60%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/Maixner2021_GraphicalAbstract.jpg")
```

**Figure 2**: **The graphical abstract of Maixner et al. (2021).**

However, because the sample was very deeply sequenced for more than 250 million paired-end reads and we do not want to wait for days for the analysis to finish, we will not use all data but just a sub-sample of them.

You can access the sub-sampled data by changing to the folder 

```bash
cd /path/to/de_novo_assembly
```

on the cluster. If you would like to repeat the practical on your own computational infrastructure, you can download the sequencing data from here:

```{bash, eval = F}
wget https://share.eva.mpg.de/index.php/s/CtLq2R9iqEcAFyg/download/2612_R1.fastq.gz
wget https://share.eva.mpg.de/index.php/s/mc5JrpDWdL4rC24/download/2612_R2.fastq.gz
```

::: {.callout-tip title="Question" appearence="simple"}

**How many sequences are in each FastQ file?**

Hint: You can run either `seqtk size <FastQ file>` or `bioawk -c fastx 'END{print NR}' <FastQ file>` to find this out.

:::

::: {.callout-note collapse="true" title="Answer"}
There are about 3.25 million paired-end sequences in these files.
:::

### Preparing the sequencing data for _de novo_ assembly

Before running the actual assembly, we need to pre-process our sequencing data. Typical pre-processing steps include the trimming of adapter sequences and barcodes from the sequencing data and the removal of host or contaminant sequences, such as the bacteriophage PhiX, which is commonly used sequenced as a quality control.

Many assembly pipelines, such as [nf-core/mag](https://nf-co.re/mag), have these steps automatically included, however, these steps can be performed prior to it, too. For this practical, I have performed these steps for us and we could directly continue with the _de novo_ assembly.

::: {.callout-important}
The characteristic of ancient DNA samples that pre-determines the success of the _de novo_ assembly is the **distribution of the DNA molecule length**. Determine this distribution prior to running the _de novo_ assembly to be able to predict the results of the _de novo_ assembly.
:::

However, since the average length of the insert size of sequencing data (**Figure 3**) is highly
correlated with the success of the assembly, we want to first evaluate it. For this we can run make
use of the program `fastp` [@Chen2018].

```{r out.width = "60%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/insert_size_scheme.png")
```

**Figure 3**: **Scheme of a read pair of Illumina sequencing data.**

Fastp will try to overlap the two mates of a read pair and, if this is possible, return the length of the merged sequence, which is identical to insert size or the DNA molecule length. If the two mates cannot be overlapped, we will not be able to know the exact DNA molecule length but only know that it is longer than 290 bp (each read has a length of 150 bp and FastP requires a 11 bp overlap between the reads).

The final histogram of the insert sizes that is returned by FastP can tell us how well preserved the DNA of an ancient sample is (**Figure 4**). The more the distribution is skewed to the right, i.e. the longer the DNA molecules are, the more likely we are to obtain long contiguous DNA sequences from the _de novo_ assembly. A distribution that is skewed to the left means that the DNA molecules are more degraded and this lowers our chances for obtaining long continuous sequences.

```{r out.width = "70%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/Orlando2013_FigS4.5.png")
```

**Figure 4**: **Example of a DNA molecule length distribution of a well-preserved ancient DNA
sample.** This histogram belongs to a 700,000-year-old horse that was preserved in permafrost
(@Orlando2013 Fig. S4).

To infer the distribution of the DNA molecules, we can run the command

```{bash, eval = F}
fastp --in1 2612_R1.fastq.gz \
      --in2 2612_R2.fastq.gz \
      --stdout --merge -A -G -Q -L --json /dev/null --html overlaps.html \
> /dev/null
```

::: {.callout-tip title="Question" appearence="simple"}

**Infer the distribution of the DNA molecule length of the sequencing data. Is this sample well-preserved?**

Hint: You can easily inspect the distribution by opening the HTML report `overlaps.html`.

:::

::: {.callout-note collapse="true" title="Answer"}

Here is the histogram of the insert sizes determined by _fastp_. By default, _fastp_ will only keep reads that are longer than 30 bp and requires an overlap between the read mates of 30 bp. The maximum read length is 150 bp, therefore, the histogram only spreads from 31 to 271 bp in total.

```{r out.width = "70%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/2612_insertsize_hist.png")
```

The sequencing data for the sample **2612** were generated across eight different sequencing runs, which differed in their nominal length. Some sequencing runs were 2x 100 bp, while others were 2x 150 bp. This is the reason why we observe two peaks just short of 100 and 150 bp. The difference to the nominal length is caused by the quality trimming of the data.

Overall, we have almost no short DNA molecules (< 50 bp) but most DNA molecules are longer than 80 bp. Additionally, there were > 200,000 read pairs that could not be overlapped. Therefore, we can conclude that the sample **2612** is moderately degraded ancient DNA sample and has many long DNA molecules.
:::

### _De novo_ assembly

Now, we will actual perform the _de novo_ assembly on the sequencing data. For this, we will use the program MEGAHIT [@LiMegahit2015], a _de Bruijn_-graph assembler. 

```{r out.width = "70%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/kmer_counting.png")
```

**Figure 5**: **Overview of inferring _k_-mers from a DNA sequencce.** Credit:
https://medium.com/swlh/bioinformatics-1-k-mer-counting-8c1283a07e29

_De Bruijn_ graph assemblers are together with overlap assemblers the pre-dominant group of assemblers. They use _k_-mers (see **Figure 5** for an example of _4_-mers) extracted from the short-read sequencing data to build the graph. For this, they connect each _k_-mer to adjacent _k_-mer using a directional edge. By walking along the edges and visiting each _k_-mer or node once, longer continuous sequences are reconstructed. This is very rough explanantion and I would advise you to watch this excerpt of a [lecture](https://www.youtube.com/watch?v=OY9Q_rUCGDw) by Rob Edwards from San Diego State Univeristy and a [Coursera lecture](https://youtu.be/TNYZZKrjCSk?t=112) by Ben Langmead from Johns Hopkins University, if you would like to learn more about it.

All _de Bruijn_ graph assemblers work in a similar way so the question is why do we use MEGAHIT and not other programs, such as metaSPAdes [@Nurk2017]?

::: {.callout-note title="Pros and cons of MEGAHIT"}

  **Pros**:

    - low-memory foot print: can be run on computational infrastructure that does not have a large memory resources 
    - can assembly both single-end and paired-end sequencing data
    - assembly algorithm can cope with the presence of high amounts of ancient DNA damage

**Cons**:

- lower assembly quality on modern comparative data, particularly a higher rate of mis-assemblies (CAMI II challenge)

:::

In the Warinnner group, we realised after some tests that MEGAHIT has its clear advantage when ancient DNA damage is present at higher quantities. While it produced a higher number of mis-assemblies compared to metaSPAdes when being evaluated on simulated modern metagenomic data (Critical Assessment of Metagenome Interpretation II challenge: @Meyer2022), it produces more long contigs when ancient DNA damage is present.

To _de novo_ assemble the short-read sequencing data of the sample **2612** using MEGAHIT, we can run the command

```{bash, eval = F}
megahit -1 2612_R1.fastq.gz \
        -2 2612_R2.fastq.gz \
        -t 14 --min-contig-len 500 \
        --out-dir megahit
```

This will use the paired-end sequencing data as input and return all contigs that are at least 500 bp long.

::: {.callout-caution}
While MEGAHIT is able to run on merged sequencing data, it is advised to use the unmerged paired-end data as input. In tests using simulated data I have observed that MEGAHIT performed slightly better when using the unmerged data and it likely has something to do with its internal algorithm to infer insert sizes from the paired-end data.
:::

While we are waiting for MEGAHIT to finish, here is a question:

::: {.callout-tip title="Question" appearence="simple"}

**Which _k_-mer lengths did MEGAHIT select for the _de novo_ assembly?**

:::

::: {.callout-note collapse="true" title="Answer"}

Based on the maximum read length, MEGAHIT decided to use the _k_-mer lengths of 21, 29, 39, 59, 79, 99, 119, and 141.
:::

Now, as MEGAHIT has finished, we want to evaluate the quality of the assembly results. MEGAHIT has written the contiguous sequences (contigs) into a single FastA file stored in the folder `megahit`. We will process this FastA file with a small script, calN50, which will count the number of contigs and give us an overview of their length distribution.

To download the script and run it, we can execute the following commands:

```{bash, eval = F}
wget https://raw.githubusercontent.com/lh3/calN50/master/calN50.js
k8 ./calN50.js megahit/final.contigs.fa
```

::: {.callout-tip title="Question" appearence="simple"}

**How many contigs were assembled? What is the sum of the lengths of all contigs? What is the maximum, the median, and the minimum contig length?**

Hint: The maximum contig length is indicated by the label "N0", the median by the label "N50", and the minimum by the label "N100"?

:::

::: {.callout-note collapse="true" title="Answer"}

MEGAHIT assembled 3,606 contigs and their lengths sum up to 11.4 Mb. The maximum contig length was 448 kb, the median length was 15.6 kb, and the minimum length was 500 bp.

:::

There is a final caveat when assembling ancient metagenomic data with MEGAHIT: while it is able to assemble sequencing data with a high percentage of C-to-T substitutions, it tends to introduce these changes into the contig sequences, too.

```{r out.width = "80%", echo = F, fig.align = "center"}
knitr::include_graphics("assets/images/chapters/denovo-assembly/Leggett2013_Fig1a.png")
```
**Figure 6**: **_De Bruijn_ graph with a bubble caused by a second allele.** Adapted from
[@Leggett2013] Figure 1 A

These C-to-T substitutions are similar to biological single-nucleotide polymorphisms in the sequencing data. Both lead the introduction of bubbles in the _de Bruijn_ graph when two alleles are present in the k-mer sequences (**Figure 6**) and the assembler decides during its pruning steps which allele to keep in the contig sequence. 

While it does not really matter which allele is kept for biological polymorphisms, it does matter for technical artefacts that are introduced by the presence of ancient DNA damage. In our group we realised that the gene sequences that were annotated on the contigs of MEGAHIT tended to have a higher number of nonsense mutations compared to the known variants of the genes. After manual inspection, we observed that many of these mutations appeared because MEGAHIT chose the damage-derived T allele over the C allele or the damage-derived A allele over a G allele (see @Klapper2023 Figure S1).

To overcome this issue, my colleagues Maxime Borry, James Fellows Yates and I developed a strategy to replace these damage-derived alleles in the contig sequences. This approach consists of aligning the short-read data against the contigs, performing genotyping along them, and finally replacing all alleles for which we have strong support for an allele that is different from the one select by MEGAHIT.

We standardised this approach and added it to the Nextflow pipeline nf-core/mag [@Krakau2022]. It can simply be activated by providing the parameter `--ancient_dna`. 

::: {.callout-caution}
While MEGAHIT is able to assemble ancient metagenomic sequencing data with high amounts of ancient DNA damage, it tends to introduce damage-derived T and A alleles into the contig sequences instead of the true C and G alleles. This can lead to a higher number of nonsense mutations in coding sequences. We strongly advise you to correct such mutations, e.g. by using the ancient DNA workflow of the Nextflow pipeline [nf-core/mag](https://nf-co.re/mag).
:::

### Aligning the short-read data against the contigs

After the assembly, the next detremental step that is required for many subsequent analyses is the alignment of the short-read sequencing data back to assembled contigs.

Analyses that require these alignment information are for example:

  - the correction of the contig sequences to remove damage-derived alleles
  - the non-reference binning of contigs into MAGs for inferring the coverage along the contigs
  - the quantification of the presence of ancient DNA damage

Aligning the short-read data to the contigs requires multiple steps:

  1. Build an index from the contigs for the alignment program BowTie2
  2. Align the short-read data against the contigs using this index with BowTie2
  3. Calculate the mismatches and deletions of the short-read data against the contig sequences
  4. Sort the aligned reads by the contig name and the coordinate they were aligned to
  5. Index the resulting alignment file for faster access

To execute the alignment step we can run the following commands:

```{bash, eval = F}
mkdir alignment
bowtie2-build -f megahit/final.contigs.fa alignment/2612
bowtie2 -p 14 â-very-sensitive -N 1 -x alignment/2612 \
    -1 2612_R1.fastq.gz -2 2612_R2.fastq.gz | \
samtools view -Sb - | \
samtools calmd -u /dev/stdin megahit/final.contigs.fa | \
samtools sort -o alignment/2612.sorted.calmd.bam -
samtools index alignment/2612.sorted.calmd.bam
```

However, these steps are rather time-consuming, even when we just have so little sequencing data as for our course example. The alignment is rather slow because we allow a single mismatch in the seeds that are used by the aligner BowTie2 to quickly determine the position of a read along the contig sequences (parameter `-N 1`). This is necessary because otherwise we might not be able to align reads with ancient DNA damage present on them. Secondly, the larger the resulting alignment file is the longer it takes to sort it by coordinate.

To save us some time and continue with the more interesting analyses, I prepared the resulting files
for us. For this, I also corrected damage-derived alleles in the contig sequences. You can access
these files on the cluster by running the following commands:

```{bash, eval = F}
mkdir alignment
ln -s /vol/volume/4c-genome-assembly/2612.sorted.calmd.bam alignment/
ln -s /vol/volume/4c-genome-assembly/2612.sorted.calmd.bam.bai alignment/
ln -s /vol/volume/4c-genome-assembly/2612.fa alignment/
```

For everyone, who runs this practical on their own infrastructure, you can download the files:

```{bash, eval = F}
mkdir alignment
wget -O alignment/2612.sorted.calmd.bam \
    https://share.eva.mpg.de/index.php/s/bDKgFLj9GpRFdPg/download/2612.sorted.calmd.bam
wget -O alignment/2612.sorted.calmd.bam.bai \
    https://share.eva.mpg.de/index.php/s/HWqg6fJj6ZEEBAL/download/2612.sorted.calmd.bam.bai
wget -O alignment/2612.fa \
    https://share.eva.mpg.de/index.php/s/z6ZAai42RPribX5/download/final.contigs.fa
```

### Reconstructing metagenome-assembled genomes



### Annotating genomes for function


```{bash, eval = F}
prokka --outdir prokka \
    --prefix 2612_003 \
    --compliant --metagenome --cpus 14 \
    metawrap_50_10_bins/bin.3.fa
```

::: {.callout-tip title="Question" appearence="simple"}

**Prokka has annotated our MAG. What type of files does Prokka return? How many genes/tRNAs/rRNAs were detected?**

Hint: Check the file `prokka/2612_003.txt` for the number of annotated elements.

:::

::: {.callout-note collapse="true" title="Answer"}

Prokka found 1,797 coding sequences, 32 tRNAs, but no rRNAs. Finding no rRNAs is a common issues when trying to assemble MAGs without long-read sequencing data and is not just characteristic for ancient DNA samples.

:::
