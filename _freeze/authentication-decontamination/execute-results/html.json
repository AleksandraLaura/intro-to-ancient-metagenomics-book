{
  "hash": "2d48a4fac6db9e9cef443f156d96c543",
  "result": {
    "markdown": "---\ntitle: Authentication and Decontamination\nauthor: Nikolay Oskolkov\n---\n\n::: {.callout-tip}\nFor this chapter's exercises, if not already performed, you will need to create the [conda environment](before-you-start.qmd#creating-a-conda-environment) from the `yml` file in the following [link](https://github.com/SPAAM-community/intro-to-ancient-metagenomics-book/raw/main/assets/envs/authentication-decontamination.yml) (right click and save as to download), and once created, activate the environment with:\n\n```bash\nconda activate authentication-decontamination\n```\n:::\n\n\n# Introduction\n\nIn ancient metagenomics we typically try to answer two questions: \"Who is there?\" and \"How ancient?\", meaning we would like to detect an organism and investigate whether this organism is ancient. There are three typical ways to identify the presence of an organism in a metagenomic sample:\n\n- alignment of DNA fragments to a reference genome ([Bowtie](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3322381/), [BWA](https://pubmed.ncbi.nlm.nih.gov/19451168/), [Malt](https://www.biorxiv.org/content/10.1101/050559v1) etc.)\n- taxonomic (kmer-based) classification of DNA fragments ([Kraken](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0), [MetaPhlan](https://www.nature.com/articles/s41587-023-01688-w), [Centrifuge](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5131823/) etc.)\n- *de-novo* genome assembly ([Megahit](https://academic.oup.com/bioinformatics/article/31/10/1674/177884), [metaSPAdes](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5411777/) etc.)\n\nThe first two are reference-based, i.e. they assume a similarity of a query ancient DNA fragment to a modern reference genome in a database. This is a strong assumption, which might not be true for very old or very diverged ancient organisms. This is the case when the reference-free *de-novo* assembly approach becomes prowerful. However, *de-novo* assembly has its own computational challenges for low-coverage ancient metagenomic samples that typically contain very short DNA fragments.\n\n![](assets/images/chapters/authentication-decontamination/metagenomic_approaches.png)\n\nWhile all the three types of metagenomic analysis are suitable for exploring composition of metagenomic samples, they do not directly validate the findings or provide information about ancient or endogenous status of the detected organims. It can happen that the detected organism\n\n1. was mis-identified (the DNA belongs to another organism than initially thought),\n2. has a modern origin (for example, lab or sequencing contaminant)\n3. is of exogenous origin (for example, an ancient microbe that entered the host *post-mortem*). \n\nTherefore, additional analysis is needed to follow-up each hit and demonstrate its ancient origin. Below, we describe a few steps that can help ancient metagenomic researchers to verify their findings and put them into biological context.\n\nIn this chapter, we will cover:\n\n- how to recognize that a detected organism was mis-identified based on breadth / evenness of coverage\n- how to validate findings by breadth of coverage filters via k-mer based taxonomic classification with KrakenUniq\n- how to validate findings using alignments and assess mapping quality, edit distance and evenness of coverage profile\n- how to detect modern contaminants via deamination profile, DNA fragmentation and post-mortem damage (PMD) scores\n- how negative (blank) controls can help disentangle ancient organisms from modern contaminants\n- how microbial source tracking can facilitate separating endogenous and exogenous microbial communities\n\nThe chapter has the following outline:\n\n- Introduction\n- Simulated ancient metagenomic data\n- Genomic hit confirmation (how we see a true-positive hit)\n  - Modern validation criteria\n    - evenness and breadth of coverage\n    - alignment quality (edit distance, mapq)\n    - affinity to reference (percent identity, multi-allelic SNPs)\n  - Ancient-specific validation\n    - deamination profile (PMD scores)\n    - DNA fragmentation\n- Microbiome contamination correction\n  - Decontamination via negative controls (blanks)\n  - Similarity to expected microbiome source (microbial source tracking)\n\n\n# Simulated ancient metagenomic data\n\nIn this chapter, we will use 10 simulated with [gargammel](https://academic.oup.com/bioinformatics/article/33/4/577/2608651) ancient metagenomic samples from Pochon et al. 2023. This data under `ameta/.\n\n![](assets/images/chapters/authentication-decontamination/aMeta.png)\n\n::: {.callout-note title=\"Self guided: data preparation\" collapse=true}\n\nThe raw simulated data can be accessed via [https://doi.org/10.17044/scilifelab.21261405](https://doi.org/10.17044/scilifelab.21261405)\n\nTo download the simulated ancient metagenomic data please use the following command lines:\n\n```bash\nmkdir ameta/ && cd ameta/\nwget https://figshare.scilifelab.se/ndownloader/articles/21261405/versions/1 \\\n&& unzip 1 && rm 1\n```\nThe DNA reads were simulated with damage, sequencing errors and Illumina adapters, therefore one will have to trim the adapters first:\n\n```bash\nfor i in $(ls *.fastq.gz)\ndo\nsample_name=$(basename $i .fastq.gz)\ncutadapt -a AGATCGGAAGAG --minimum-length 30 -o ${sample_name}.trimmed.fastq.gz ${sample_name}.fastq.gz -j 4\ndone\n```\n\nNow, after the basic data pre-processing has been done, we can proceed with validation, authentication and decontamination analyses.\n:::\n\n# Genomic hit confirmation\n\nOnce an organism has been detected in a sample (via alignment, classification or *de-novo* assembly), one needs to take a closer look at multiple quality metrics in order to reliably confrm that the organism is not a false-positive detection and is of ancient origin. The methods used for this purpose can be divided into modern validation and ancient-specific validation criteria. Below, we will cover both of them.\n\n\n## Modern validation criteria\n\nThe modern validation methods aim at confirming organism presence regradless of its ancient status. The main approaches include evenness / breadth of coverage computation, assessing alignmnet quality, and monitoring affinity of the DNA reads to the reference genome of the potential host.\n\n\n### Depth vs breadth and evenness of coverage\n\nConcluding organism presence by relying solely on the numbers of assigned sequenced reads (aka depth of coverage metric) turns out to be not optimal and too permissive, which may result in a large amount of false-positive discoveries. For example, when using alignment to a reference genome, the mapped reads may demonstrate non-uniform coverage as visualized in the [Integrative Genomics Viewer (IGV)](https://software.broadinstitute.org/software/igv/) below.\n\n![](assets/images/chapters/authentication-decontamination/IGV_uneven_coverage_Y.pestis.png)\n\nIn this case, DNA reads originating from another microbe were (mis-)aligned to *Yersina pestis* reference genome. It can be observed that a large numer the reads align only to a few conserved genomic loci. Therefore, even if many thousands of DNA reads are capable of aligning to the reference genome, the overall uneven alignment pattern suggests no presence of *Yersina pestis* in the metagenomic sample. Thus, not only the number of assigned reads (proportinal to depth of coverage metric) but also the **breadth and evenness of coverage** metrics become of particular importance for veryfication of metagenomic findings, i.e. hits with DNA reads uniformly aligned across the reference genome are more likely to be true-positive detections.\n\n![](assets/images/chapters/authentication-decontamination/depth_vs_breadth_of_coverage.png)\n\nIn the next sections, we will show how to practically compute the breadth and evenness of coverage via KrakenUniq and Samtools.\n\n\n### Breadth of coverage via KrakenUniq\n\nHere we are going to demonstrate that one can assess breadth of coverage information already at the taxonomic profiling step. Although taxonomic classifiers do not perform alignment, some of them, such as [KrakenUniq](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-018-1568-0) and [Kraken2](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0) provide a way to infer breadth of coverage in addition to the number of assigned reads to a taxon. This allows for immediate filtering out a lot of false positive hits. Since Kraken-family classifiers are typically faster and [less memory-demanding](https://www.biorxiv.org/content/10.1101/2022.06.01.494344v1.full.pdf), i.e. can work with very large reference databases, compared to genome aligners, they provide a robust and fairly unbiased initial taxonomic profiling, which can still later be followed-up with proper alignment and computing evenness of coverage as described above.\n\n::: {.callout-note title=\"Self guided: data preparation\" collapse=true}\n\n⚠️ This step will require large amounts of memory and CPUs!, so if running yourself please note this step is better suited for a server, HPC cluster, or the cloud rather than on a laptop! \n\nTo profile the data with KrakenUniq one needs a database, a pre-built complete microbial NCBI RefSeq database can be accessed via [https://doi.org/10.17044/scilifelab.21299541](https://doi.org/10.17044/scilifelab.21299541). \n\nPlease use the following command line to download the database:\n\n```bash\nwget https://figshare.scilifelab.se/ndownloader/articles/21299541/versions/1 \\\n&& unzip 1 && rm 1\n```\n\nThe following example command is how you would execute KrakenUniq.\n\n```bash\n#| eval: false\n\nfor i in $(ls *.trimmed.fastq.gz)\ndo\nkrakenuniq --db KRAKENUNIQ_DB --fastq-input $i --threads 20 \\\n--classified-out${i}.classified_sequences.krakenuniq \\\n--unclassified-out ${i}.unclassified_sequences.krakenuniq \\\n--output ${i}.sequences.krakenuniq --report-file ${i}.krakenuniq.output\ndone\n```\n\n:::\n\nTaxonomic k-mer-based classification of the ancient metagenomic reads can be done via KrakenUniq. However as this requires a very large database file, we have already made available for you the output files from this step.\n\nKrakenUniq by default delivers a proxy metric for breadth of coverage called the **number of unique kmers** (in the 4th column of its output table) assigned to a taxon. KrakenUniq output can be easily filtered with respect to both depth and breadth of coverage, which substantially reduces the number of false-positive hits.\n\n![](assets/images/chapters/authentication-decontamination/krakenuniq_filter.png)\n\nWe can filter the KrakenUniq output with respect to both depth (*taxReads*) and breadth (*kmers*) of coverage with the following custom Python script, which selects only species with at east 200 assigned reads and 1000 unique k-mers. After the filtering, we can see a *Yersinia pestis* hit in the *sample 10* that passess the filtering thresholds with respect to both depth and breadth of coverage.\n\n```bash\nfor i in $(ls *.krakenuniq.output)\ndo\n$SCRIPTS_DIR/filter_krakenuniq.py $i 1000 200 $SCRIPTS_DIR/pathogenomesFound.tab\ndone\n```\n\n![](assets/images/chapters/authentication-decontamination/filtered_krakenuniq_output.png)\n\n\nWe can also easily produce a KrakenUniq taxonomic abundance table *krakenuniq_abundance_matrix.txt* using the custom R script below, which takes as argument the folder *KRAKENUNIQ* containing the KrakenUniq output files. From the *krakenuniq_abundance_matrix.txt* table, it becomes clear that *Yersinia pestis* seems to be present in a few other samples in addition to sample 10.\n\n```bash\nRscript ${SCRIPTS_DIR}/krakenuniq_abundance_matrix.R KRAKENUNIQ \\ \nKRAKENUNIQ_ABUNDANCE_MATRIX 1000 200\n```\n\n![](assets/images/chapters/authentication-decontamination/krakenuniq_abundance_matrix.png)\n\nWhile KrakenUniq delivers information about breadth of coverage by default, one has to use a special flag *--report-minimizer-data* when running Kraken2 in order to get the breadth of coverage proxy which is called the **number of distrinct minimizers** for the case of Kraken2. Below, we provide an example Kraken2 command line containing the distinct minimizer flag:\n\n```bash\nDBNAME=Kraken2_DB_directory\nKRAKEN_INPUT=sample.fastq.gz\nKRAKEN_OUTPUT=Kraken2_output_directory\nkraken2 --db $DBNAME --fastq-input $KRAKEN_INPUT --threads 20 \\\n--classified-out $KRAKEN_OUTPUT/classified_sequences.kraken2 \\\n--unclassified-out $KRAKEN_OUTPUT/unclassified_sequences.kraken2 \\\n--output $KRAKEN_OUTPUT/sequences.kraken2 \\\n--report $KRAKEN_OUTPUT/kraken2.output \\\n--use-names --report-minimizer-data\n```\n\nThen the filtering of Kraken2 output with respect to breadth and depth of coverage can be done by analogy with filtering KrakenUniq output table. In case of *de-novo* assembly, the original DNA reads are typically alligned back to the assembled contigs, and the evennes / breadth of coverage can be computed from these alignments.\n\n\n### Evenness of coverage via Samtools\n\nNow, after we have detected an interesting *Y. pestis* hit, we would like to follow it up, and compute multiple quality metrics (including proper breadth and evenness of coverage) from alignments (Bowtie2 aligner willl be used in our case) of the DNA reads to the *Y. pestis* reference genome. Below, we download *Yersinia pestis* reference genome from NCBI, build its Bowtie2 index, and align trimmed reads against *Yersinia pestis* reference genome with Bowtie2. Do not forget to sort and index the alignments as it will be important for computing the evenness of coverage. It is also recommended to remove multi-mapping reads, i.e. the ones that have MAPQ = 0, at least for Bowtie and BWA aligners that are commonly used in ancient metagenomics. Samtools with *-q* flag can be used to extract reads with MAPQ > = 1.\n\n```bash\nNCBI=https://ftp.ncbi.nlm.nih.gov; ID=GCF_000222975.1_ASM22297v1\nwget $NCBI/genomes/all/GCF/000/222/975/${ID}/${ID}_genomic.fna.gz\n\ngunzip ${ID}_genomic.fna.gz; echo NC_017168.1 > region.bed\nseqtk subseq ${ID}_genomic.fna region.bed > NC_017168.1.fasta\nbowtie2-build --large-index NC_017168.1.fasta NC_017168.1.fasta --threads 20\n\nbowtie2 --large-index -x NC_017168.1.fasta --end-to-end --threads 20 \\ \n--very-sensitive -U sample10.trimmed.fastq.gz | samtools view -bS -h -q 1 \\\n-@ 20 - > Y.pestis_sample10.bam\nsamtools sort Y.pestis_sample10.bam -@ 20 > Y.pestis_sample10.sorted.bam\nsamtools index Y.pestis_sample10.sorted.bam\n```\n\nNext, the breadth / evenness of coverage can be computed from the BAM-alignments via *samtools depth* as follows:\n\n```bash\nsamtools depth -a Y.pestis_sample10.sorted.bam > Y.pestis_sample10.sorted.boc\n```\nand visualized using for example the following R code snippet (alternatively [aDNA-BAMPlotter](https://github.com/MeriamGuellil/aDNA-BAMPlotter) can be used):\n\n```R\n# Read output of samtools depth commans\ndf <- read.delim(\"Y.pestis_sample10.sorted.boc\", header = FALSE, sep = \"\\t\")\nnames(df) <- c(\"Ref\", \"Pos\", \"N_reads\")\n\n# Split reference genome in tiles, compute breadth of coverage for each tile\nN_tiles <- 500\nstep <- (max(df$Pos) - min(df$Pos)) / N_tiles\ntiles <- c(0:N_tiles) * step; boc <- vector()\nfor(i in 1:length(tiles))\n{\n  df_loc <- df[df$Pos >= tiles[i] & df$Pos < tiles[i+1], ]\n  boc <- append(boc, rep(sum(df_loc$N_reads > 0) / length(df_loc$N_reads),\n  dim(df_loc)[1]))\n}\nboc[is.na(boc)]<-0; df$boc <- boc\nplot(df$boc ~ df$Pos, type = \"s\", xlab = \"Genome position\", ylab = \"Coverage\")\nabline(h = 0, col = \"red\", lty = 2)\nmtext(paste0(round((sum(df$N_reads > 0) / length(df$N_reads)) * 100, 2), \n\"% of genome covered\"), cex = 0.8)\n```\n\n![](assets/images/chapters/authentication-decontamination/Evenness_of_coverage.png)\n\nIn the R script above, we simply split the reference genome into *N_tiles* tiles and compute the breadth of coverage (number of reference nucleotides covered by at least one read normalized by the total length) locally in each tile. By visualizing how the local breadth of coverage changes from tile to tile, we can monitor the distribution of the reads across the reference genome. In the evenness of coverage figure above, the reads seem to cover all parts of the reference genome uniformly, which is a good evidence of true-positive detection, even though the total mean breadth of coverage is low due to the low total number of reads.\n\n\n### Alignment quality\n\nIn addition to evenness and breadth of coverage, it is very informative to monitor how well the metagenomic reads map to a reference genome. Here one can control for **mapping quality** ([MAPQ](https://samtools.github.io/hts-specs/SAMv1.pdf) field in the BAM-alignments) and the number of mismatches for each read, i.e. **edit distance**.\n\nMapping quality (MAPQ) can be extracted from the 5th column of BAM-alignments using Samtools and *cut* command in bash.\n\n```bash\nsamtools view Y.pestis_sample10.sorted.bam | cut -f5 > mapq.txt\n```\n\nThen the 5th column of the filtered BAM-alignment can be visualized via a simple histogram in R as below for two random metagenomic samples.\n\n```R\nhist(as.numeric(readLines(\"mapq.txt\")), col = “darkred”, breaks = 100)\n```\n\n![](assets/images/chapters/authentication-decontamination/MAPQ.png)\n\nNote that MAPQ scores are computed slightly differently for Bowtie and BWA, so they are not directly comparable, however, for both MAPQ ~ 10-30, as in the histograms below, indicates good affinity of the DNA reads to the reference genome. here we provide some examples of how typical MAPQ histograms for Bowtie2 and BWA alignments can look like:\n\n![](assets/images/chapters/authentication-decontamination/mapq.png)\n\n\nEdit distance can be extracted by gathering information in the NM-tag inside BAM-alignemnts, which reports the number of mismatches for each aligned read. This can be done either in bash / awk, or using handy functions from *Rsamtools* R package:\n\n```R\nlibrary(\"Rsamtools\")\nparam <- ScanBamParam(tag = \"NM\")\nbam <- scanBam(\"Y.pestis_sample10.sorted.bam\", param = param)\nbarplot(table(bam[[1]]$tag$NM), ylab=\"Number of reads\", xlab=\"Number of mismatches\")\n```\n\n![](assets/images/chapters/authentication-decontamination/edit_distance.png)\n\n\nIn the barplot above we can see that the majority of reads align either without or with very few mismatches, which is an evidence of high affinity of the aligned reads with respect to the reference genome. For a true-positive finding, the edit distance barplot typically has a decreasing profile. However, for a very degraded DNA, it can have a mode around 1 or 2, which can also be reasonable. A fasle-positive hit would have a mode of the edit distance barplot shifted toward higher numbers of mismatches.\n\n\n### Affinity to reference\n\nVery related to edit distance is another alignment validation metric which is called **percent identity**. It represents a barplot demonstrating the numbers of reads that are 100% identical to the reference genome (i.e. map without a single mismatch), 99% identical, 98% identical etc. Misaligned reads originating from another related organism have typically most reads with percent identity of 93-96%. In the figure below, the panels (c–e) demonstrate different percent identity distributions. In panel c, most reads show a high similarity to the reference, which indicates a correct assignment of the reads to the reference. In panel d, most reads are highly dissimilar to the reference, which suggests that they originate from different related species. In some cases, as in panel e, a mixture of correctly assigned and misassigned reads can be observed. \n\n![](assets/images/chapters/authentication-decontamination/multialleleicSNPs.png)\n\nAnother important way to detect reads that cross-map between related species is **haploidy** or checking the amount of **multi-allelic SNPs**. Because bacteria are haploid organisms, only one allele is expected for each genomic position. Only a small number of multiallelic sites are expected, which can result from a few misassigned or incorrectly aligned reads. In the figure above, panels (f–i) demonstrate histograms of SNP allele frequency distributions. Panel f demonstrates the situation when we have only a few multiallelic sites originating from a misaligned reads. This is a preferrable case scenario corresponding to correct assignment of the reads to the reference. Please also check the corresponding \"Good alignments\" IGV visualization to the right in the figure above.\n\nIn contrast, a large number of multiallelic sites indicates that the assigned reads originate from more than one species or strain, which can result in symmetric allele frequency distributions (e.g., if two species or strains are present in equal abundance) (panel g) or asymmetric distributions (e.g., if two species or strains are present in unequal abundance) (panel h). A large number of misassigned reads from closely related species can result in a large number of multiallelic sites with low frequencies of the derived allele (panel i). The situations (g-i) correspond to incorrect assignment of the reads to the reference. Please also check the corresponding \"Bad alignments\" IGV visualization to the right in the figure above.\n\n\n\n## Ancient-specific validation criteria\n\nIn contrast to modern genomic hit validation criteria, the ancient-specific validation methods concentrate on DNA degradation and damage pattern as ultimate signs of ancient DNA. Below, we will discuss demination profile, read length distribution and post mortem damage (PMD) scores metrics that provide good confirmation of ancient origin of the detected organism.\n\n\n### Ancient status\n\nChecking evenness of coverage and alignment quality can help us to make sure that the organism we are thinking about is really present in the metagenomic sample. However, we still need to address the question \"How ancinet?\". For this purpose we need to compute **deamination profile** and **read length distribution** of the aligned reads in order to prove that they demonstrate damage pattern and are sufficiently fragmented, which would be a good evidence of ancient origin of the detected organisms. \n\nDeamination profile of a damaged DNA demonstrate an enrichment of C / T polymorphisms at the ends of the reads compared to all other single nucleotide substitutions. There are several tools for computing demination profile, but perhaps the most popular is [mapDamage](https://academic.oup.com/bioinformatics/article/29/13/1682/184965). The tool can be run using the following command line:\n\n```bash\nmapDamage -i Y.pestis_sample10.sorted.bam -r NC_017168.1.fasta \\\n-d MAPDAMAGE --merge-reference-sequences --no-stats\n```\n\n![](assets/images/chapters/authentication-decontamination/deamination.png)\n\nmaDamage delivers a bunch of useful statistics, among other read length distribution can be checked. A typical mode of DNA reads should be within a range 30-70 base-pairs in order to be a good evidence of DNA fragmentation. Reads longer tha 100 base-pairs are more likely to originate from modern contamination.\n\n![](assets/images/chapters/authentication-decontamination/read_length.png)\n\nAnother useful tool that can be applied to assess how DNA is damaged is [PMDtools](https://github.com/pontussk/PMDtools) which is a maximum-likelihood probabilistic model that calculates an ancient score, **PMD score**, for each read. The ability of PMDtools to infer ancient status with a single read resolution is quite unique and different from mapDamage that can only assess deamination based on a number of reads. PMD scores can be computed using the following command line, please note that Python2 is needed for this purpose.\n\n```bash\nsamtools view -h Y.pestis_sample10.bam | python2 ${SCRIPTS_DIR}/pmdtools.0.60.py \\\n --printDS > PMDscores.txt\n```\n\nThe distribution of PMD scores can be visualized via a histogram in R as follows:\n\n```R\npmd_scores <- read.delim(\"PMDscores.txt\", header = FALSE, sep = \"\\t\")\nhist(pmd_scores$V4, breaks = 1000, xlab = \"PMDscores\")\n```\n\n![](assets/images/chapters/authentication-decontamination/pmd_scores.png)\n\nTypcally, reads with PMD scores greater than 3 are considered to be reliably ancient, i.e. damaged, and can be extracted for taking a closer look. Therefore PMDtools is great for separating ancient reads from modern contaminant reads.\n\nAs mapDamage, PMDtools can also compute demination profile. However, the advantage of PMDtools that it can compute deamination profile for UDG / USER treated samples (with the flag *--CpG*). For this purpose, PMDtools uses only CpG sites which escape the treatment, so deamination is not gone completely and there is a chance to authenticate treated samples. Computing deamination pattern with PMDtoools can be achieved with the following command line (please note that the scripts *pmdtools.0.60.py* and *plotPMD.v2.R* can be downloaded from the github repository here https://github.com/pontussk/PMDtools):\n\n```bash\nsamtools view Y.pestis_sample10.bam | python2 ${SCRIPTS_DIR}/pmdtools.0.60.py \\ \n--platypus > PMDtemp.txt\n\nR CMD BATCH plotPMD.v2.R\n```\n\n![](assets/images/chapters/authentication-decontamination/PMD_Skoglund_et_al_2015_Current_Biology.png)\n\nWhen performing ancient status analysis on **de-novo** assembled contigs, it can be computationally challenging and time consuming to run mapDamage or PMDtools on all of them as there can be hundreds of thousands contigs. In addition, the outputs from mapDamage and PMDtools lacking a clear numeric quantitiy or a statistical test that could provide an \"ancient vs. non-ancient\" desicion for each **de-novo** assembled contig. To address these limitations, [pyDamage](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8323603/pdf/peerj-09-11845.pdf) tool was recently developed. PyDamage evaluates the amount of aDNA damage and tests the hypothesis whether a model assuming presence of aDNA damage better explains the data than a null model.\n\n![](assets/images/chapters/authentication-decontamination/pyDamage.png)\n\npyDamage can be run on a sorted BAM-alignments of the microbial reads to the **de-novo** assembled contigs using the following command line:\n\n```bash\npydamage analyze -w 30 -p 14 filtered.sorted.bam\n```\n\n# Microbiome contamination correction\n\nModern contamination can severely bias ancient metagenomic analysis. Also, ancient contamination, i.e. entered *post-mortem*, can potentially lead to false biological interpretations. Therefore, a lot of efforts in the ancient metagenomics field are directed on establishing methodology for identification of contaminants. Among them, the use of negative (blank) control samples is perhaps the most reliable and straighforward method. Additionally, one often performs microbial source tracking for predicting environment (including contamination environment) of origin for ancient metagenomic samples.\n\n## Decontamination\n\nModern contamination is one of the major problems in ancient metagenomics analysis. Large fractions of modern bacterial, animal or human DNA in metagenomic samples can lead to false biological and historical conclusions. A lot of [scientific literature](https://onlinelibrary.wiley.com/doi/10.1002/bies.202000081) is dedicated to this topic, and comprehensive tables and sources of potential contamination (e.g. animal and bacterial DNA present in PCR reagensts) are available.\n\n![](assets/images/chapters/authentication-decontamination/contaminants_literature.png)\n\nA good practice to discriminate between endogenous and contaminant organisms is to sequence negative controls, so-called **blanks**. Organisms detected on blanks, like the microbial genera reported in the table below, can substantially facilitate making more informed decision about true metagenomic profile of a sample. Nevertheless, the table below may seem rather conservative since in addition to well-known environmental contaminants as *Burkholderia* and *Pseudomonas* it includes also human oral genera as *Streptococcus*, which are probably less likely to be of environmental origin.\n\n![](assets/images/chapters/authentication-decontamination/contaminants_list.png)\n\nIt is typically assumed that an organism found on a blank has a lower confidence to be endogenous to the studied metagenomic sample, and sometimes it is even expluded from the downstream analysis as an unreliable hit. Despite there are attempts to automate filtering out modern contaminants (we will discuss them below), decontamination process still remains to be a tidious manual work where each candidate should be carefully investigated from different contexts in order to prove its ancient and endogenous origin.\n\nIf negative control samples (balnks) are available, contaminating organisms can be detected by comparing their abundances in the negative controls with true samples. In this case, contaminant organisms stand out by their high prevalence in both types of samples if one simply plots mean across samples abundance of each detected organism in true samples and negative controls against each other as in the figure below:\n\n```R\nsamples<-read.delim(\"krakenuniq_abundance_matrix.txt\",header=TRUE,\nrow.names = 1, check.names = FALSE, sep = \"\\t\")\ncontrols<-read.delim(\"blank_krakenuniq_abundance_matrix.txt\",header=TRUE,\nrow.names = 1, check.names = FALSE, sep = \"\\t\")\n\ndf <- merge(samples, controls, all = TRUE, by = \"row.names\")\nrownames(df)<-df$Row.names; df$Row.names <- NULL; df[is.na(df)] <- 0\n\ntrue_sample <- subset(df,select=colnames(df)[!grepl(\"control\",colnames(df))])\nnegative_control <- subset(df,select=colnames(df)[grepl(\"control\",colnames(df))])\n\nplot(log10(rowMeans(true_sample)+1) ~ log10(rowMeans(negative_control)+1),\nxlab = \"Log10 ( Negative controls )\", ylab = \"Log10 ( True samples )\",\nmain = \"Organism abundance in true samples vs. negative controls\",\npch = 19, col = \"blue\")\n\npoints(log10(rowMeans(true_sample)+1)[(log10(rowMeans(true_sample)+1) > 1) & \n(log10(rowMeans(negative_control)+1)>1)] ~ \nlog10(rowMeans(negative_control)+1)[(log10(rowMeans(true_sample)+1) > 1) & \n(log10(rowMeans(negative_control)+1)>1)], pch = 19, col = \"red\")\n\ntext(log10(rowMeans(true_sample)+1)[(log10(rowMeans(true_sample)+1) > 1) & \n(log10(rowMeans(negative_control)+1) > 1)] ~ \nlog10(rowMeans(negative_control)+1)[(log10(rowMeans(true_sample)+1) > 1) & \n(log10(rowMeans(negative_control)+1)  >1)],\nlabels = rownames(true_sample)[(log10(rowMeans(true_sample)+1) > 1) & \n(log10(rowMeans(negative_control)+1) > 1)], pos = 4)\n```\n\n![](assets/images/chapters/authentication-decontamination/blank_decontam.png)\n\nIn the figure above, one point indicates an organism detected in a group of metagenomic samples. The points highlighted by red have high abundance in negative control samples, and therefore they are likely contamiannts.\n\nIn addition to PCR reagents and lab contaminants, reference databses can also be contaminanted by various, often microbial, organisms. A typical example that when screening environmental or sedimentary ancient DNA samples, a fish *Cyprinos carpio* can pop up if adapter trimming procedure was not successful for some reason.\n\n![](assets/images/chapters/authentication-decontamination/carpio.png)\n\nIt was noticed that the *Cyprinos carpio* reference genome available at NCBI contains large fraction of Illumina sequncing adapters. Therefore, appearence of this organism in your analysis may falsely lead your conclusion toward potential lake or river present in the excavation site.\n\n\nLet us now discuss a few available computational approaches to decontaminate metagenomic samples. One of them is [decontam](https://microbiomejournal.biomedcentral.com/articles/10.1186/s40168-018-0605-2) R package that offers a simple statistical test for whether a detected organism is likely contaminant. This approach is useful when DNA quantitation data recording the concentration of DNA in each sample (e.g. PicoGreen fluorescent intensity measures) is available. The idea of the *decontam* is that contaminant DNA is expected to be present in approximately equal and low concentrations across samples, while sample DNA concentrations can vary widely. As a result, the expected frequency of contaminant DNA varies inversely with total sample DNA concentration (red line in the figure below), while the expected frequency of non-contaminant DNA does not (blue line).\n\n![](assets/images/chapters/authentication-decontamination/decontam.png)\n\nAnother popular tool for detecting contaminating microorganisms is [Recentrifuge](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006967). It works as a classifier that is trained to recognize contaminant microbial organisms. In case of Recentrifuge, one has to use blanks or other negative controls and provide microbial names and abundances on the blanks in order to train Recentrifuge to recognize endogenous vs. contaminant sources.\n\nIf one wants to assess the degree of contamination for each sample, there is a handy tool [cuperdec](https://github.com/jfy133/cuperdec), which is an R package that allows a quick comparison of microbial profiles in a query metagenomic sample against a database. The idea of *cuperdec* is to rank organisms in each sample by their abundance and then using an \"expanding window\" approach to compute their enrichment in a reference database that contains a comprehensive list of microbial organisms which are specific to a tissue / environment in question. The tool produces so-called *Cumulative Percent Decay* curves that aim to represent the level of endogenous content of microbiome samples, such as ancient dental calculus, to help to identify samples with low levels of preservation that should be discarded for downstream analysis.\n\n```R\nlibrary(\"cuperdec\"); library(\"magrittr\"); library(\"dplyr\")\n\n# Load database (in this case oral database)\ndata(cuperdec_database_ex)\ndatabase <- load_database(cuperdec_database_ex, target = \"oral\") %>% print()\n\n# Load abundance matrix and metadata\ntaxatable <- load_taxa_table(\"krakenuniq_abundance_matrix.txt\")  %>% print()\nmetadata <- as_tibble(data.frame(Sample = unique(taxatable$Sample), \nSample_Source = \"Oral\"))\n\n# Compute cumulative percent decay curves, filter and plot results\ncurves <- calculate_curve(taxatable, database = database) %>% print()\nfilter_result <- simple_filter(curves, percent_threshold = 50) %>% print()\nplot_cuperdec(curves, metadata = metadata, filter_result)\n```\n\n![](assets/images/chapters/authentication-decontamination/cuperdec2.png)\n\nIn the figure above, one curve represents one sample, and the red curves have a very high amount of contamination and very low amount of endogenous DNA. These samples might be considered to be dropped from the downstream analysis.\n\n## Microbial source tracking\n\nFor the case of ancient microbiome profiling, in addition to traditional inspection of the list of detected organisms and comparing it with the ones detected on blanks, we can use tools that make a prediction on what environment the detected organisms most likely come from. \n\nThe most popular and widely used tool is called [**SourceTracker**](https://www.nature.com/articles/nmeth.1650#citeas). SourceTracker is a Bayesian version of the Gaussian Mixture Model (GMM) clustering algorithm that is trained on a user-supplied reference data called **Sources**, i.e. different classes such as Soil or Human Oral or Human Gut microbial communities etc., and then it can estimate proportion / contribution of each of these sources the users actual samples called **Sinks**.\n\n![](assets/images/chapters/authentication-decontamination/SourceTracker.png)\n\nOriginally, SourceTracker was developed for 16S data, i.e. using only 16S ribosomal RNA genes, but it can be easily trained using also shotgun metagenomics data, which was demonstrated in its metagenomic extension called [mSourceTracker](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7100590/) and its faster and more scalable version [FEAST](https://www.nature.com/articles/s41592-019-0431-x). The input data for SourceTracker are metadata, i.e. each sample has to have \"source\" or \"sink\" annotation as well as environmental label (e.g. Oral, Gut, Soil etc.), and microbial abundances (OTU abundances) quantified in some way, for example through [QIIME](https://qiime2.org/) pipeline, MetaPhlan or Kraken. The SourceTracker R script can be downloaded from [https://github.com/danknights/sourcetracker](https://github.com/danknights/sourcetracker).\n \n Sourcetracker expects two input data frames: metadata with at least sample name, environment and source / sink labels, and abundance matrix. Note that source and sink metadata and abundances have to be merged together prior to using SourceTracker. Here we are going to use data from the [Human Microbiome Project (HMP)](https://hmpdacc.org/) as sources, and we are going to merge the HMP data with the sink samples into single OTU table and meta-data table.\n\n```R\notus_hmp <- read.delim(\"otus_hmp.txt\", header = TRUE, row.names = 1, sep = \"\\t\")\nmeta_hmp <- read.delim(\"meta_hmp.txt\", header = TRUE, row.names = 1, sep = \"\\t\")\n\notus_sink<-read.delim(\"krakenuniq_abundance_matrix.txt\",header=T,row.names=1,sep=\"\\t\")\notus <- merge(otus_hmp, otus_sink, all = TRUE, by = \"row.names\")\nrownames(otus) <- otus$Row.names; otus$Row.names <- NULL; otus[is.na(otus)] <- 0\nmeta_sink <- data.frame(ID = colnames(otus_sink), Env = \"Unknown\", SourceSink = \"sink\")\nrownames(meta_sink) <- meta_sink$ID; meta_sink$ID<-NULL\nmetadata <- rbind(meta_hmp, meta_sink)\n\notus <- as.data.frame(t(as.matrix(otus)))\notus[otus > 0] <- 1; otus <- otus[rowSums(otus)!=0,]\nmetadata<-metadata[as.character(metadata$Env)!=\"Vaginal\",]; envs <- metadata$Env\ncommon.sample.ids <- intersect(rownames(metadata), rownames(otus))\notus <- otus[common.sample.ids,]; metadata <- metadata[common.sample.ids,]\n```\n\nNext, training SourceTracker on source samples and running predictions on sink samples can be done using following command lines:\n\n```R\n# Train SourceTracker on sources (HMP) and run predictions on sinks\nsource('/sourcetracker/src/SourceTracker.r')\ntrain.ix <- which(metadata$SourceSink=='source')\ntest.ix <- which(metadata$SourceSink=='sink')\nst <- sourcetracker(otus[train.ix,], envs[train.ix])\nresults <- predict(st, otus[test.ix,], alpha1 = 0.001, alpha2 = 0.001)\n```\n\nFinally, we can plot SourceTracker environment inference in the form of barcharts as follows:\n\n```R\n# Sort SourceTracker proportions for plotting\nprops <- results$proportions\nprops <- props[order(-props[,\"Oral\"]),]\nresults$proportions <- props\n\n# Prepare SourceTracker output for plotting\nname <- rep(rownames(results$proportions), each = 4)\nvalue <- as.numeric(t(results$proportions))\nlabels <- c(\"Gut\",\"Oral\",\"Skin\",\"Unknown\"); condition<-rep(labels, length(test.ix))\ndata <- data.frame(name, condition, value)\n\n# Plot SourceTracker inference as a barplot\nlibrary(\"ggplot2\")\nggplot(data, aes(fill=condition, y=value, x = reorder(name, seq(1:length(name))))) + \ngeom_bar(position = \"fill\", stat = \"identity\") + \ntheme(axis.text.x = element_text(angle = 90, size=5, hjust=1, vjust=0.5)) + \nxlab(\"Sample\") + ylab(\"Fraction\")\n```\n\n![](assets/images/chapters/authentication-decontamination/source_tracker1.png)\n\nIn the figure above the SourceTracker was trained on Human Microbiome Project (HMP) data, and was capable of predicting the fractions of oral, gut, skin or other microbial composition on the query sink samples. In a similar way, environmental soil or marine microbes can be used as Sources. In this way, environmental percentage of contamination can be detected per sample.\n\nA drawback of SourceTracker, mSourceTracker and FEAST is that they require a microbial abundance table after a taxonomic classification with e.g. QIIME or Kraken has been performed. Such taxonomic classification can be biased since it is computed against a reference database with known taxonomic annotation. In contrast, a novel microbial source tracking tool [decOM](https://www.biorxiv.org/content/10.1101/2023.01.26.525439v1.full) aims at moving away from database-dependent methods and using unsupervised approaches exploiting read-level sequence composition.\n\n![](assets/images/chapters/authentication-decontamination/deCOM.png)\n\n*decOM* uses [kmtricks](https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac029/6576015) to compute a matrix of k-mer counts on raw reads (FASTQ files) from source samples, and then uses the source k-mer abundance matrix for looking up k-mer composition of sink samples. This allows *decOM* to calculate microbial contributions / fractions from the sources. For example, for estimating contributions from ancient Oral (aOral), modern Oral (mOral), Skin and Sediment / Soil environments one can use an already computed source matrix from here [https://github.com/CamilaDuitama/decOM/](https://github.com/CamilaDuitama/decOM/) and provide it as a *-p_sources* parameter. \n\n\n::: {.callout-note title=\"Self guided: data preparation\" collapse=true}\n\nIf doing a self-guided tutorial, you will need to download a rather large pre-built database.\n\n```bash\n\ngit clone https://github.com/CamilaDuitama/decOM.git  \ncd decOM  \nconda env create -n decOM --file environment.yml  \nconda deactivate\nconda activate decOM  \n\nexport PATH=/absolute/path/to/decOM:${PATH}\n\n# Prepare input fof-files that have a key - value format\ncd CUTADAPT # folder containing trimmed fastq-files\nfor i in {1..10}\ndo\necho \"sample${i}_trimmed : sample${i}_trimmed.fastq.gz\" > sample${i}_trimmed.fof\necho sample${i}_trimmed >> FASTQ_NAMES_LIST.txt\ndone\n\n# Download pre-built kmer-matrix of sources (aOral, mOral, Sediment/Soil, Skin)\nwget https://zenodo.org/record/6513520/files/decOM_sources.tar.gz  \ntar -xf decOM_sources.tar.gz\n```\n:::\n\nThe following example command is how you would execute deCOM. However as this requires a very large database file, we have already made available for you the output files from this step.\n\n\n```{bash}\n#| eval: false\n\n# DO NOT RUN: Run decOM predictions\ndecOM -p_sources decOM_sources/ -p_sinks FASTQ_NAMES_LIST.txt \\\n-p_keys decOM/FASTQ -mem 900GB -t 15\n```\n\n\n\nIn the command line used to generate, he *-p_sinks* parameter provides a list of sink samples, for example *SRR13355807*.\nThe sink fastq-files are placed in *decOM/FASTQ* together with *keys* fof-files containing the mapping between fastq file names and locations of the fastq-files, for example *SRR13355807 : decOM/FASTQ/SRR13355807.fastq.gz*. The contributions from the sources to the sink samples, which are recorded in the *decOM_output.csv* output file, can then be processed and plotted as follows:\n\n```R\ndf<-read.csv(\"decOM_output.csv\", check.names=FALSE)\n\nresult <- subset(df, select = c(\"Sink\", \"Sediment/Soil\", \"Skin\", \"aOral\", \n\"mOral\", \"Unknown\"))\nrownames(result) <- result$Sink; result$Sink <- NULL\nresult <- result / rowSums(result)\nresult<-result[order(-result$aOral),]\n\nname <- rep(rownames(result), each = 5); value <- as.numeric(t(result))\ncondition <- rep(c(\"Sediment/Soil\",\"Skin\",\"aOral\",\"mOral\",\"Unknown\"), \ndim(result)[1])\ndata <- data.frame(name, condition, value)\n\nlibrary(\"ggplot2\"); library(\"viridis\")\nggplot(data, aes(fill=condition, y=value, x=reorder(name,seq(1:length(name))))) + \n  geom_bar(position = \"fill\", stat = \"identity\") + \n  theme(axis.text.x = element_text(angle=90, size = 5, hjust = 1, vjust = 0.5)) + \n  xlab(\"Sample\") + ylab(\"Fraction\")\n```\n\n![](assets/images/chapters/authentication-decontamination/decOM1.png)\n\n*decOM* has certain advantages compared to *SourceTracker* as its is a taxonomic classification / database free approach. However, it also appears to be very sensitive to the particular training / source data set. In the example above it can be seen that the microbial source tracking of sink samples is very much dominated by the Oral community, which was the training / source data set.\n\n\n## Summary\n\nIn this chapter we have learned that:\n\n- Evenness of coverage is an important metric for validation of findings\n\n- Deamination profile, DNA fragmentation, mapping quality, edit distance and PMD scores are other authentication / validation metrics to consider\n\n- Negative controls are important for disentangling ancient / endogenous from modern / exogenous contamination\n\n- Microbial source tracking is another layer of evidence that can facilitate interpretation of ancient metagenomic findings\n\n\n## Questions to think about\n\n1. What is a false-positive microbial finding and how can we recognize it?\n\n2. What is the diffeerence between depth, breadth and evenness of coverage?\n\n3. What is contamination and how can it bias ancient metagenomic analysis?\n\n4. How can we separate ancient from modern DNA sequence?\n\n5. What is a negative (blank) control sample and why is it useful to have?\n\n6. What is microbial source tracking and how can it help with decontamination?\n\n\n## Readings\n\n1. Clio Der Sarkissian, Irina M. Velsko, Anna K. Fotakis, Åshild J. Vågene, Alexander Hübner, and James A. Fellows Yates, Ancient Metagenomic Studies: Considerations for the Wider Scientific Community, mSystems 2021 Volume 6  Issue 6  e01315-21.\n\n2. Warinner C, Herbig A, Mann A, Fellows Yates JA, Weiß CL, Burbano HA, Orlando L, Krause J. A Robust Framework for Microbial Archaeology. Annu Rev Genomics Hum Genet. 2017 Aug 31;18:321-356. doi: 10.1146/annurev-genom-091416-035526. Epub 2017 Apr 26. PMID: 28460196; PMCID: PMC5581243.\n\n3. Orlando, L., Allaby, R., Skoglund, P. et al. Ancient DNA analysis. Nat Rev Methods Primers 1, 14 (2021). https://doi.org/10.1038/s43586-020-00011-0\n\n\n\n## Resources\n\n1. **KrakenUniq**: Breitwieser, F. P., Baker, D. N., & Salzberg, S. L. (2018). KrakenUniq: confident and fast metagenomics classification using unique k-mer counts. Genome Biology, vol. 19(1), p. 1–10. http://www.ec.gc.ca/education/default.asp?lang=En&n=44E5E9BB-1\n\n2. **Samtools**: Heng Li, Bob Handsaker, Alec Wysoker, Tim Fennell, Jue Ruan, Nils Homer, Gabor Marth, Goncalo Abecasis, Richard Durbin, 1000 Genome Project Data Processing Subgroup, The Sequence Alignment/Map format and SAMtools, Bioinformatics, Volume 25, Issue 16, 15 August 2009, Pages 2078–2079, https://doi.org/10.1093/bioinformatics/btp352\n\n3. **PMDtools**: Skoglund P, Northoff BH, Shunkov MV, Derevianko AP, Pääbo S, Krause J, Jakobsson M. Separating endogenous ancient DNA from modern day contamination in a Siberian Neandertal. Proc Natl Acad Sci U S A. 2014 Feb 11;111(6):2229-34. doi: 10.1073/pnas.1318934111. Epub 2014 Jan 27. PMID: 24469802; PMCID: PMC3926038.\n\n4. **pyDamage**: Borry M, Hübner A, Rohrlach AB, Warinner C. PyDamage: automated ancient damage identification and estimation for contigs in ancient DNA de novo assembly. PeerJ. 2021 Jul 27;9:e11845. doi: 10.7717/peerj.11845. PMID: 34395085; PMCID: PMC8323603.\n\n5. **SourceTracker**: Knights D, Kuczynski J, Charlson ES, Zaneveld J, Mozer MC, Collman RG, Bushman FD, Knight R, Kelley ST. Bayesian community-wide culture-independent microbial source tracking. Nat Methods. 2011 Jul 17;8(9):761-3. doi: 10.1038/nmeth.1650. PMID: 21765408; PMCID: PMC3791591.\n\n6. **deCOM**: https://www.biorxiv.org/content/10.1101/2023.01.26.525439v1, doi: https://doi.org/10.1101/2023.01.26.525439\n\n7. **aMeta**: https://www.biorxiv.org/content/10.1101/2022.10.03.510579v1, doi: https://doi.org/10.1101/2022.10.03.510579\n\n8. **Bowtie2**: Langmead, B., Salzberg, S. Fast gapped-read alignment with Bowtie 2. Nat Methods 9, 357–359 (2012). https://doi.org/10.1038/nmeth.1923\n\n9. **cuperdec**: https://cran.r-project.org/web/packages/cuperdec/index.html\n\n10. **decontam**: https://www.bioconductor.org/packages/release/bioc/html/decontam.html\n\n",
    "supporting": [
      "authentication-decontamination_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}